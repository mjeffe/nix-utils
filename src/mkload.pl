#!/usr/bin/perl
# ---------------------------------------------------------------------------
# $Id: mkload.pl 120 2012-03-15 10:29:58Z mjeffe $
#
# Another bug-ugly utility from Matt's code archive.
#
# TODO:
# add ability to load fixed width files
# for postgresql see: 
#   http://www.postgresonline.com/journal/archives/157-Import-fixed-width-data-into-PostgreSQL-with-just-PSQL.html
# ---------------------------------------------------------------------------

use warnings;
use strict;

use IO::Handle;
use File::Basename;
use Getopt::Std;
my $this = basename($0);

my $USAGE = "usage: $this -d delimiter -t tablename [-o mysql|postgresql] data_file\n"
          . "where: -o (output database) defaults to postgresql\n";

# prototypes
sub output_mysql();
sub output_postgresql();


if ( $#ARGV+1 < 1 ) {
   print STDERR $USAGE;
   exit(1);
}

use vars qw($opt_d $opt_t $opt_o $opt_h);
getopts('d:t:o:h');
print $USAGE if ($opt_h);
my $delimiter = $opt_d || ",";
my $table = $opt_t || "my_table_name";
my $output_db = $opt_o || "postgresql";

my $file = shift;
my $sqlfile = "load_" . $table . "." . $output_db . ".sql";

# open input and output files
if ( $file =~ m/.gz$/ ) {
   open(INPUT, "gzip -dcf $file |") or die "Can't open input file: $file\n";
} else {
   open(INPUT, "<", $file) or die "Can't open input file: $file\n";
}
open(OUT, ">", $sqlfile) or die "Can't open output file: $sqlfile\n";

# get the header row, parse out and normalize field names
my @fields;
while (my $line = <INPUT>) {
   # normalize the field names
   $line =~ s/\r?\n$//;   # chomp only chmops $/
   $line =~ s/"//g;
   $line =~ s/[^a-zA-Z0-9$delimiter]/_/g;
   $line =~ tr/A-Z/a-z/;

   @fields = split(quotemeta($delimiter), $line);

   # just need the header
   last;
}

# get field lengths.  Hack using dtof.
my $lenfile = ".tmp_" . $table . ".layout";
system("dtof -l -d '$delimiter' -o $lenfile $file > /dev/null");
if ($? == -1) {
   print "failed to execute: $!\n";
}
   elsif ($? & 127) {
   print "problem calculating field lengths using dtof: $!";
   #printf "child died with signal %d, %s coredump\n", ($? & 127),  ($? & 128) ? ’with’ : ’without’;
   exit(1);
}

open(LEN, "<", $lenfile) or die "Can't open $lenfile\n";
my @lengths;
while (my $line = <LEN>) {
   chomp($line);
   my @row = split(/,\s/, $line);
   push(@lengths, $row[2]);
}
close(LEN);
unlink($lenfile);

if ( $output_db =~ m/^my/i ) {
   output_mysql();
} else {
   output_postgresql();
}


# ---------------------------------------------------------------------------
# output sql for mysql
sub output_mysql() {

   print OUT <<END;
-- generated by $this for $output_db;

drop table if exists $table;

create table $table
(
END

   my $i = 0;
   foreach my $field (@fields) {
      print OUT "   $field varchar(";
      print OUT ($lengths[$i] eq '0') ? "1" : $lengths[$i];
      print OUT ")";
      if (scalar @fields > $i+1) {
         print OUT ",\n";
      } else {
         print OUT "\n";
      }
      $i++;
   }
   #print OUT "   " . join(" varchar(50),\n   ", @fields) . " varchar(50)\n";

   print OUT <<END;
);

load data local infile '$file'
into table $table
fields terminated by '$delimiter' enclosed by '"'
END

   print OUT <<'END';
lines terminated by '\r\n'
ignore 1 lines;

END

   print OUT "analyze table $table;\n\n";
}


# ---------------------------------------------------------------------------
# output sql for postgresql database
sub output_postgresql() {

   print OUT <<END;
-- generated by $this for $output_db;
--
-- use the following command to load your file:
--
--   psql -U<user> -d<dbname> -f $sqlfile
--

\\timing
-- NOTE: the entire load opperation will rollback (even ddl) if antying fails
\\set ON_ERROR_STOP
\\set AUTOCOMMIT off

drop table if exists $table;

create table $table
(
END

   my $i = 0;
   foreach my $field (@fields) {
      print OUT "   $field varchar(";
      print OUT ($lengths[$i] eq '0') ? "1" : $lengths[$i];
      print OUT ")";
      if (scalar @fields > $i+1) {
         print OUT ",\n";
      } else {
         print OUT "\n";
      }
      $i++;
   }
   #print OUT "   " . join(" varchar(50),\n   ", @fields) . " varchar(50)\n";

   print OUT <<END;
);

\\copy $table from '$file' with delimiter as '$delimiter' null as '' csv header

-- postgresql has transactional ddl, so without the commit not even the table will be created!
commit;

analyze $table;

END
}

1;

